{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR0SKBDbaOqR"
      },
      "source": [
        "# MP5: Training Your Diffusion Model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryMrLOORbWLz"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdFQ6c9-Pm4Y"
      },
      "outputs": [],
      "source": [
        "# Import essential modules. Feel free to add whatever you need.\n",
        "import matplotlib.pyplot as plt\n",
        "import torch # added\n",
        "from torch import optim # added\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fagaomPqYfxv"
      },
      "source": [
        "## Visualization helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qCe1sbsTlSC"
      },
      "outputs": [],
      "source": [
        "def visualize_images_with_titles(images: torch.Tensor, column_names: list[str]):\n",
        "    \"\"\"\n",
        "    Visualize images as a grid and title the columns with the provided names.\n",
        "\n",
        "    Args:\n",
        "        images: (N, C, H, W) tensor of images, where N is (number of rows * number of columns)\n",
        "        column_names: List of column names for the titles.\n",
        "\n",
        "    Example usage:\n",
        "    visualize_images_with_titles(torch.randn(16, 1, 32, 32), ['1', '2', '3', '4'])\n",
        "    \"\"\"\n",
        "    num_images, num_columns = images.shape[0], len(column_names)\n",
        "    assert num_images % num_columns == 0, 'Number of images must be a multiple of the number of columns.'\n",
        "\n",
        "    num_rows = num_images // num_columns\n",
        "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(num_columns * 1, num_rows * 1))\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.axis('off')\n",
        "        if i < num_columns:\n",
        "            ax.set_title(column_names[i % num_columns])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZZziMrxiIk"
      },
      "source": [
        "# Part 1: Training a Single-step Denoising UNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dokvxybn_DwK"
      },
      "source": [
        "## Implementing Simple and Composed Ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhpEzgwCJqbW"
      },
      "outputs": [],
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.gelu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class DownConv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.gelu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.gelu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=7)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.avg_pool(x)\n",
        "\n",
        "\n",
        "class Unflatten(nn.Module):\n",
        "    def __init__(self, in_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=7, stride=1, padding=0)\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.gelu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(in_channels, out_channels)\n",
        "        self.conv2 = Conv(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.downconv = DownConv(in_channels, out_channels)\n",
        "        self.convblock = ConvBlock(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.downconv(x)\n",
        "        x = self.convblock(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.upconv = UpConv(in_channels, out_channels)\n",
        "        self.convblock = ConvBlock(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.upconv(x)#handling imput channels properly\n",
        "        x = self.convblock(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsntLNl8PkdG"
      },
      "source": [
        "## Implementing Unconditional UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94h1Q3BxN0ha"
      },
      "outputs": [],
      "source": [
        "class UnconditionalUNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_hiddens: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.initial_conv = Conv(in_channels, num_hiddens)\n",
        "        self.down1 = DownBlock(num_hiddens, num_hiddens * 2)\n",
        "        self.down2 = DownBlock(num_hiddens * 2, num_hiddens * 4)\n",
        "\n",
        "        self.flatten = Flatten()#bottlenecking\n",
        "        self.unflatten = Unflatten(num_hiddens * 4)\n",
        "        self.up1 = UpBlock(num_hiddens * 8, num_hiddens * 2)\n",
        "        self.up2 = UpBlock(num_hiddens * 4, num_hiddens)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(num_hiddens * 2, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
        "        x0 = self.initial_conv(x)\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.flatten(x2)\n",
        "        x4 = self.unflatten(x3)\n",
        "        x5 = torch.cat([x4, x2], dim=1)\n",
        "        x6 = self.up1(x5)\n",
        "\n",
        "        x7 = torch.cat([x6, x1], dim=1)\n",
        "        x8 = self.up2(x7)\n",
        "        x9 = torch.cat([x8, x0], dim=1)\n",
        "        out = self.final_conv(x9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itmHM7udTlSD"
      },
      "source": [
        "## Visualizing the noising process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFhyA6joTlSD"
      },
      "outputs": [],
      "source": [
        "def add_noise(images, sigma):\n",
        "    \"\"\"Add Gaussian noise to images.\n",
        "\n",
        "    Args:\n",
        "        images: Clean images tensor of shape (N, C, H, W)\n",
        "        sigma: Standard deviation of the Gaussian noise\n",
        "\n",
        "    Returns:\n",
        "        Noisy images tensor of shape (N, C, H, W)\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(images) * sigma\n",
        "    return images + noise\n",
        "\n",
        "\n",
        "dataset = MNIST(root=\"data\", download=True, transform=ToTensor(), train=True)\n",
        "sigmas = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
        "num_examples = 5\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "for i, sample_idx in enumerate(range(num_examples)):\n",
        "    image, _ = dataset[sample_idx]\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    for j, sigma in enumerate(sigmas):\n",
        "        noisy_image = add_noise(image, sigma)\n",
        "        plt.subplot(num_examples, len(sigmas), i * len(sigmas) + j + 1)\n",
        "        plt.imshow(noisy_image[0, 0].numpy(), cmap='gray')\n",
        "\n",
        "        if i == 0:\n",
        "            plt.title(f\"σ = {sigma}\")\n",
        "\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Varying noise levels on MNIST digits\", y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4mNfCUETlSE"
      },
      "source": [
        "## Training a Single-Step Unconditional UNet\n",
        "\n",
        "- Plot the loss curve\n",
        "- Sample results on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFnYvvmWTlSE"
      },
      "outputs": [],
      "source": [
        "def train_unconditional_unet(num_epochs=5, sigma=0.5, batch_size=256):\n",
        "    # Data loaders\n",
        "    train_dataset = MNIST(root='data', download=True, transform=ToTensor(), train=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = MNIST(root='data', download=True, transform=ToTensor(), train=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = UnconditionalUNet(in_channels=1, num_hiddens=128).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    batch_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (clean_images, _) in enumerate(train_loader):\n",
        "            clean_images = clean_images.to(device)\n",
        "\n",
        "            noisy_images = add_noise(clean_images, sigma)\n",
        "            denoised_images = model(noisy_images)\n",
        "            loss = F.mse_loss(denoised_images, clean_images)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}')\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        print(f'Epoch: {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.6f}')\n",
        "\n",
        "        if epoch == 0 or epoch == num_epochs - 1:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                clean_images, _ = next(iter(test_loader))\n",
        "                clean_images = clean_images[:10].to(device)\n",
        "                noisy_images = add_noise(clean_images, sigma)\n",
        "\n",
        "                denoised_images = model(noisy_images)\n",
        "                comparison = torch.cat([clean_images, noisy_images, denoised_images], dim=0)\n",
        "\n",
        "                column_names = ['Input', 'Noisy (σ=0.5)', 'Output']\n",
        "                visualize_images_with_titles(comparison.cpu(), column_names)\n",
        "\n",
        "    #plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(batch_losses)\n",
        "    plt.title('Training Losses')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unconditional_model = train_unconditional_unet(num_epochs=5, sigma=0.5)"
      ],
      "metadata": {
        "id": "zPM7nZv_2kYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFvFEa0uTlSF"
      },
      "source": [
        "## Out-of-Distribution Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUb-JMHrTlSF"
      },
      "outputs": [],
      "source": [
        "def test_out_of_distribution(model, sigmas=[0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]):\n",
        "    \"\"\"Test the model on different noise levels.\"\"\"\n",
        "    test_dataset = MNIST(root='data', download=True, transform=ToTensor(), train=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "    clean_images, _ = next(iter(test_loader))\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    clean_images = clean_images.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    num_samples = len(clean_images)\n",
        "    num_sigmas = len(sigmas)\n",
        "\n",
        "    fig, axes = plt.subplots(num_sigmas * 2, num_samples, figsize=(num_samples * 1.2, num_sigmas * 2.4))\n",
        "    for i, sigma in enumerate(sigmas):\n",
        "        #adding noise\n",
        "        with torch.no_grad():\n",
        "            noisy_images = add_noise(clean_images, sigma)\n",
        "            denoised_images = model(noisy_images)\n",
        "\n",
        "        for j in range(num_samples):\n",
        "            ax_noisy = axes[i*2, j]\n",
        "            ax_noisy.imshow(noisy_images[j, 0].cpu().numpy(), cmap='gray')\n",
        "            ax_noisy.axis('off')\n",
        "\n",
        "            if j == 0:\n",
        "                ax_noisy.set_title(f\"Noisy (σ={sigma})\")\n",
        "            ax_denoised = axes[i*2+1, j]\n",
        "            ax_denoised.imshow(denoised_images[j, 0].cpu().numpy(), cmap='gray')\n",
        "            ax_denoised.axis('off')\n",
        "            if j == 0:\n",
        "                ax_denoised.set_title(f\"Denoised (σ={sigma})\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "test_out_of_distribution(unconditional_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ObcO_jUamVE"
      },
      "source": [
        "# Part 2: Training a Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMI3IMkjayxQ"
      },
      "source": [
        "## Implementing a Time-conditioned UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkchbyYkzAvV"
      },
      "outputs": [],
      "source": [
        "class FCBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_channels, out_channels)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.linear2 = nn.Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.gelu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TimeConditionalUNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        num_hiddens: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.initial_conv = Conv(in_channels, num_hiddens)\n",
        "\n",
        "        self.down1 = DownBlock(num_hiddens, num_hiddens * 2)\n",
        "        self.down2 = DownBlock(num_hiddens * 2, num_hiddens * 4)\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        self.unflatten = Unflatten(num_hiddens * 4)\n",
        "        self.up1 = UpBlock(num_hiddens * 8, num_hiddens * 2)\n",
        "        self.up2 = UpBlock(num_hiddens * 4, num_hiddens)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(num_hiddens * 2, in_channels, kernel_size=1)\n",
        "\n",
        "        self.fc1_t = FCBlock(1, num_hiddens * 4)\n",
        "        self.fc2_t = FCBlock(1, num_hiddens * 2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        t: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "            t: (N,) normalized time tensor.\n",
        "\n",
        "        Returns:\n",
        "            (N, C, H, W) output tensor.\n",
        "        \"\"\"\n",
        "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
        "        t = t.view(-1, 1)\n",
        "        x0 = self.initial_conv(x)\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "\n",
        "        x3 = self.flatten(x2)\n",
        "        x4 = self.unflatten(x3)\n",
        "\n",
        "        #for debug time embed\n",
        "        t1 = self.fc1_t(t)\n",
        "        t2 = self.fc2_t(t)\n",
        "\n",
        "        t1 = t1.view(-1, self.num_hiddens * 4, 1, 1)\n",
        "        x4 = x4 + t1\n",
        "\n",
        "        x5 = torch.cat([x4, x2], dim=1)\n",
        "        x6 = self.up1(x5)\n",
        "        t2 = t2.view(-1, self.num_hiddens * 2, 1, 1)\n",
        "        x6 = x6 + t2\n",
        "\n",
        "        x7 = torch.cat([x6, x1], dim=1)\n",
        "        x8 = self.up2(x7)\n",
        "        x9 = torch.cat([x8, x0], dim=1)\n",
        "        out = self.final_conv(x9)#(N, in_channels, 28, 28)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nyxOM-RbZnC"
      },
      "source": [
        "## Implementing DDPM Forward and Inverse Process for Time-conditioned Denoising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIvMw63T6JkE"
      },
      "outputs": [],
      "source": [
        "def ddpm_schedule(beta1: float, beta2: float, num_ts: int) -> dict:\n",
        "    \"\"\"Constants for DDPM training and sampling.\n",
        "\n",
        "    Arguments:\n",
        "        beta1: float, starting beta value.\n",
        "        beta2: float, ending beta value.\n",
        "        num_ts: int, number of timesteps.\n",
        "\n",
        "    Returns:\n",
        "        dict with keys:\n",
        "            betas: linear schedule of betas from beta1 to beta2.\n",
        "            alphas: 1 - betas.\n",
        "            alpha_bars: cumulative product of alphas.\n",
        "    \"\"\"\n",
        "    assert beta1 < beta2 < 1.0, \"Expect beta1 < beta2 < 1.0.\"\n",
        "\n",
        "    betas = torch.linspace(beta1, beta2, num_ts)\n",
        "    alphas = 1 - betas\n",
        "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
        "    return {\n",
        "        'betas': nn.Parameter(betas, requires_grad=False),\n",
        "        'alphas': nn.Parameter(alphas, requires_grad=False),\n",
        "        'alpha_bars': nn.Parameter(alpha_bars, requires_grad=False)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfvtHEFf_7Q3"
      },
      "outputs": [],
      "source": [
        "def ddpm_forward(\n",
        "    unet: TimeConditionalUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    x_0: torch.Tensor,\n",
        "    num_ts: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Algorithm 1 of the DDPM paper.\n",
        "\n",
        "    Args:\n",
        "        unet: TimeConditionalUNet\n",
        "        ddpm_schedule: dict\n",
        "        x_0: (N, C, H, W) input tensor.\n",
        "        num_ts: int, number of timesteps.\n",
        "    Returns:\n",
        "        (,) diffusion loss.\n",
        "    \"\"\"\n",
        "    unet.train()\n",
        "\n",
        "    device = x_0.device\n",
        "    batch_size = x_0.shape[0]\n",
        "    t = torch.randint(1, num_ts + 1, (batch_size,), device=device)\n",
        "    epsilon = torch.randn_like(x_0)\n",
        "    alpha_bars = ddpm_schedule['alpha_bars']\n",
        "    alpha_bars_t = alpha_bars[t - 1].view(-1, 1, 1, 1)\n",
        "    x_t = torch.sqrt(alpha_bars_t) * x_0 + torch.sqrt(1 - alpha_bars_t) * epsilon\n",
        "    t_normalized = (t - 1) / (num_ts - 1)#normalized\n",
        "\n",
        "    epsilon_theta = unet(x_t, t_normalized)\n",
        "\n",
        "    loss = F.mse_loss(epsilon_theta, epsilon)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNE8-455IDm3"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def ddpm_sample(\n",
        "    unet: TimeConditionalUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    img_wh: tuple[int, int],\n",
        "    num_ts: int,\n",
        "    seed: int = 0,\n",
        "):\n",
        "    unet.eval()\n",
        "    torch.manual_seed(seed)\n",
        "    device = next(unet.parameters()).device\n",
        "    batch_size = 10\n",
        "\n",
        "    betas = ddpm_schedule['betas']\n",
        "    alphas = ddpm_schedule['alphas']\n",
        "    alpha_bars = ddpm_schedule['alpha_bars']\n",
        "\n",
        "    x_t = torch.randn(batch_size, unet.in_channels, *img_wh, device=device)\n",
        "\n",
        "    all_samples = [x_t.cpu()]\n",
        "\n",
        "    for t in range(num_ts, 0, -1):\n",
        "        t_normalized = torch.full((batch_size,), (t - 1) / (num_ts - 1), device=device)\n",
        "        z = torch.randn_like(x_t) if t > 1 else torch.zeros_like(x_t)\n",
        "        with torch.no_grad():\n",
        "            predicted_noise = unet(x_t, t_normalized)\n",
        "\n",
        "        alpha_t = alphas[t - 1]\n",
        "        alpha_bar_t = alpha_bars[t - 1]\n",
        "        beta_t = betas[t - 1]\n",
        "\n",
        "        x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * predicted_noise) / torch.sqrt(alpha_bar_t)\n",
        "\n",
        "        x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
        "        if t > 1:\n",
        "            alpha_bar_t_minus_1 = alpha_bars[t - 2]\n",
        "            coef1 = torch.sqrt(alpha_bar_t_minus_1) * beta_t / (1 - alpha_bar_t)\n",
        "            coef2 = torch.sqrt(alpha_t) * (1 - alpha_bar_t_minus_1) / (1 - alpha_bar_t)\n",
        "            x_t = coef1 * x_0_pred + coef2 * x_t + torch.sqrt(beta_t) * z\n",
        "        else:\n",
        "            x_t = x_0_pred\n",
        "\n",
        "        if t % (num_ts // 10) == 0 or t == 1:\n",
        "            all_samples.append(x_t.cpu())\n",
        "\n",
        "    final_samples = x_t\n",
        "    all_samples = torch.stack(all_samples, dim=1)\n",
        "\n",
        "    return final_samples, all_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_hVifFyw20j"
      },
      "outputs": [],
      "source": [
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unet: TimeConditionalUNet,\n",
        "        betas: tuple[float, float] = (1e-4, 0.02),\n",
        "        num_ts: int = 300,\n",
        "        p_uncond: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.unet = unet\n",
        "        self.num_ts = num_ts\n",
        "        self.p_uncond = p_uncond\n",
        "\n",
        "        self.ddpm_schedule = nn.ParameterDict(ddpm_schedule(betas[0], betas[1], num_ts))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "\n",
        "        Returns:\n",
        "            (,) diffusion loss.\n",
        "        \"\"\"\n",
        "        return ddpm_forward(\n",
        "            self.unet, self.ddpm_schedule, x, self.num_ts\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def sample(\n",
        "        self,\n",
        "        img_wh: tuple[int, int],\n",
        "        seed: int = 0,\n",
        "    ):\n",
        "        return ddpm_sample(\n",
        "            self.unet, self.ddpm_schedule, img_wh, self.num_ts, seed\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMYZt6IZTlSH"
      },
      "source": [
        "## Training the Time-conditioned UNet\n",
        "\n",
        "- Plot the loss curve\n",
        "- Sample results on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIuV2xECTlSH"
      },
      "outputs": [],
      "source": [
        "def train_time_conditioned_unet(num_epochs=20, batch_size=128):\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0.5, 0.5)\n",
        "    ])\n",
        "    train_dataset = MNIST(root='data', download=True, transform=transform, train=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = TimeConditionalUNet(in_channels=1, num_classes=10, num_hiddens=64).to(device)\n",
        "\n",
        "    ddpm = DDPM(model, betas=(1e-4, 0.02), num_ts=300).to(device)\n",
        "\n",
        "    print(\"Testing time embedding...\")#for debug\n",
        "    test_t = torch.linspace(0, 1, 5).to(device)\n",
        "    for t in test_t:\n",
        "        t_batch = torch.ones(2, device=device) * t\n",
        "        test_input = torch.randn(2, 1, 28, 28).to(device)\n",
        "        test_out = model(test_input, t_batch)\n",
        "        print(f\"Time {t.item():.2f}: Output range: {test_out.min().item():.2f} to {test_out.max().item():.2f}\")\n",
        "\n",
        "    optimizer = optim.Adam(ddpm.parameters(), lr=2e-4)#1e-3 was leading to slow convergence\n",
        "\n",
        "    gamma = 0.1**(1.0 / num_epochs)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "    batch_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ddpm.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (clean_images, _) in enumerate(train_loader):\n",
        "            clean_images = clean_images.to(device)\n",
        "            loss = ddpm(clean_images)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        print(f'Epoch: {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.6f}')\n",
        "        #visualization code\n",
        "        if epoch in [0, 4, 9, 14, 19]:\n",
        "            with torch.no_grad():\n",
        "                ddpm.eval()\n",
        "                num_rows = 4\n",
        "                num_cols = 10\n",
        "                fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, num_rows), facecolor='black')\n",
        "\n",
        "                fig.patch.set_facecolor('black')\n",
        "\n",
        "                for col in range(num_cols):\n",
        "                    for row in range(num_rows):\n",
        "                        samples, _ = ddpm.sample(img_wh=(28, 28), seed=epoch + row * 100 + col)\n",
        "                        sample = (samples[0, 0].cpu().numpy() + 1) / 2\n",
        "                        axs[row, col].imshow(sample, cmap='gray')\n",
        "                        axs[row, col].axis('off')\n",
        "                        if row == 0:\n",
        "                            axs[row, col].set_title(f\"Sample {col+1}\", color='white')\n",
        "\n",
        "                plt.suptitle(f\"Epoch {epoch+1}\", color='white', fontsize=16)\n",
        "                plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "                plt.savefig(f\"time_conditioned_samples_epoch_{epoch+1}.png\", facecolor='black')\n",
        "                plt.show()\n",
        "\n",
        "                samples, animation = ddpm.sample(img_wh=(28, 28), seed=epoch)\n",
        "                animation = (animation + 1) / 2\n",
        "\n",
        "                plt.figure(figsize=(12, 3), facecolor='black')\n",
        "                num_frames = animation.shape[1]\n",
        "                frames_to_show = min(8, num_frames)\n",
        "                skip = num_frames // frames_to_show\n",
        "\n",
        "                for i in range(frames_to_show):\n",
        "                    idx = i * skip\n",
        "                    if idx >= num_frames:\n",
        "                        idx = num_frames - 1\n",
        "                    plt.subplot(1, frames_to_show, i + 1)\n",
        "                    plt.imshow(animation[0, idx, 0].cpu().numpy(), cmap='gray')\n",
        "                    plt.axis('off')\n",
        "                    if i == 0:\n",
        "                        plt.title(\"Noise\", color='white')\n",
        "                    elif i == frames_to_show - 1:\n",
        "                        plt.title(\"Final\", color='white')\n",
        "                    else:\n",
        "                        plt.title(f\"Step {idx}\", color='white')\n",
        "                plt.suptitle(f'Sampling process after epoch {epoch+1}', color='white')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"time_conditioned_process_epoch_{epoch+1}.png\", facecolor='black')\n",
        "                plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(batch_losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return ddpm\n",
        "\n",
        "tmodel = train_time_conditioned_unet(num_epochs=20, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_conditioned_diffusion_gif(model, sample_index=0, output_dir=\"time_diffusion_gifs\", epochs=[0, 4, 9, 14, 19]):\n",
        "    \"\"\"\n",
        "    Create a GIF showing the diffusion process (noise to image) for time-conditioned generation.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for epoch in epochs:\n",
        "        print(f\"Creating diffusion process GIF for epoch {epoch+1}\")\n",
        "        samples, animation = model.sample(img_wh=(28, 28), seed=epoch)\n",
        "        animation = (animation + 1) / 2\n",
        "        frames = []\n",
        "        num_frames = animation.shape[1]\n",
        "\n",
        "        for t in range(num_frames):\n",
        "            fig, ax = plt.subplots(figsize=(3, 3))\n",
        "            ax.imshow(animation[0, t, 0].cpu().numpy(), cmap='gray')\n",
        "\n",
        "            if t == 0:\n",
        "                stage = \"Noise\"\n",
        "            elif t == num_frames - 1:\n",
        "                stage = \"Final\"\n",
        "            else:\n",
        "                stage = f\"Step {t}\"\n",
        "\n",
        "            ax.set_title(f\"Sample {sample_index}: {stage}\")\n",
        "            ax.axis('off')\n",
        "\n",
        "            fig.canvas.draw()\n",
        "            frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
        "            plt.close(fig)\n",
        "\n",
        "            frames.append(frame)\n",
        "        gif_path = os.path.join(output_dir, f\"sample_{sample_index}_process_epoch_{epoch+1}.gif\")\n",
        "        imageio.mimsave(gif_path, frames, duration=0.1)#0.1 seconds per frame for faster animation\n",
        "        print(f\"Saved GIF to {gif_path}\")"
      ],
      "metadata": {
        "id": "xhTVTpPvrC7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio tqdm"
      ],
      "metadata": {
        "id": "zcqkurYxq5IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_time_conditioned_diffusion_gif(tmodel, sample_index=0)\n"
      ],
      "metadata": {
        "id": "TQ6RXkEFsTh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW2FBjpn8CTZ"
      },
      "source": [
        "### Implementing class-conditioned UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAXZYlOt8Rzy"
      },
      "outputs": [],
      "source": [
        "class ClassConditionalUNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        num_hiddens: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_classes = num_classes\n",
        "        self.initial_conv = Conv(in_channels, num_hiddens)\n",
        "        self.down1 = DownBlock(num_hiddens, num_hiddens * 2)\n",
        "        self.down2 = DownBlock(num_hiddens * 2, num_hiddens * 4)\n",
        "        self.flatten = Flatten()#bottleneck 1x1\n",
        "        self.unflatten = Unflatten(num_hiddens * 4)\n",
        "\n",
        "        self.up1 = UpBlock(num_hiddens * 8, num_hiddens * 2)\n",
        "        self.up2 = UpBlock(num_hiddens * 4, num_hiddens)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(num_hiddens * 2, in_channels, kernel_size=1)\n",
        "\n",
        "        self.fc1_t = FCBlock(1, num_hiddens * 4)\n",
        "        self.fc1_c = FCBlock(num_classes, num_hiddens * 4)\n",
        "        self.fc2_t = FCBlock(1, num_hiddens * 2)\n",
        "        self.fc2_c = FCBlock(num_classes, num_hiddens * 2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        c: torch.Tensor,\n",
        "        t: torch.Tensor,\n",
        "        mask: torch.Tensor = None,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "            c: (N,) int64 condition tensor.\n",
        "            t: (N,) normalized time tensor.\n",
        "            mask: (N,) mask tensor. If not None, mask out condition when mask == 0.\n",
        "\n",
        "        Returns:\n",
        "            (N, C, H, W) output tensor.\n",
        "        \"\"\"\n",
        "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
        "\n",
        "        t = t.view(-1, 1)#(N, 1)\n",
        "        c_one_hot = F.one_hot(c, num_classes=self.num_classes).float()\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.view(-1, 1)\n",
        "            c_one_hot = c_one_hot * mask\n",
        "\n",
        "        x0 = self.initial_conv(x)\n",
        "\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.flatten(x2)\n",
        "        x4 = self.unflatten(x3)\n",
        "\n",
        "        t1 = self.fc1_t(t)\n",
        "        c1 = self.fc1_c(c_one_hot)\n",
        "        t2 = self.fc2_t(t)\n",
        "        c2 = self.fc2_c(c_one_hot)\n",
        "\n",
        "        t1 = t1.view(-1, self.num_hiddens * 4, 1, 1)\n",
        "        c1 = c1.view(-1, self.num_hiddens * 4, 1, 1)\n",
        "        t2 = t2.view(-1, self.num_hiddens * 2, 1, 1)\n",
        "        c2 = c2.view(-1, self.num_hiddens * 2, 1, 1)\n",
        "        x4 = c1 * x4 + t1#class conditioning algo\n",
        "\n",
        "        x5 = torch.cat([x4, x2], dim=1)\n",
        "        x6 = self.up1(x5)\n",
        "        x6 = c2 * x6 + t2\n",
        "\n",
        "        x7 = torch.cat([x6, x1], dim=1)\n",
        "        x8 = self.up2(x7)\n",
        "        x9 = torch.cat([x8, x0], dim=1)\n",
        "\n",
        "        out = self.final_conv(x9)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NobmVh4U8BRP"
      },
      "outputs": [],
      "source": [
        "def ddpm_forward(\n",
        "    unet: ClassConditionalUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    x_0: torch.Tensor,\n",
        "    c: torch.Tensor,\n",
        "    p_uncond: float,\n",
        "    num_ts: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Algorithm 3 of the DDPM paper - Class-conditioned training.\n",
        "\n",
        "    Args:\n",
        "        unet: ClassConditionalUNet\n",
        "        ddpm_schedule: dict\n",
        "        x_0: (N, C, H, W) input tensor (clean image).\n",
        "        c: (N,) int64 condition tensor (class labels).\n",
        "        p_uncond: float, probability of unconditioning the condition.\n",
        "        num_ts: int, number of timesteps.\n",
        "\n",
        "    Returns:\n",
        "        (,) diffusion loss.\n",
        "    \"\"\"\n",
        "    unet.train()\n",
        "\n",
        "    device = x_0.device\n",
        "    batch_size = x_0.shape[0]\n",
        "    mask = torch.bernoulli(torch.ones(batch_size, device=device) * (1 - p_uncond))\n",
        "    t = torch.randint(1, num_ts + 1, (batch_size,), device=device)#step 6 of algo 3\n",
        "\n",
        "    epsilon = torch.randn_like(x_0)\n",
        "\n",
        "    alpha_bars = ddpm_schedule['alpha_bars']\n",
        "    alpha_bars_t = alpha_bars[t - 1].view(-1, 1, 1, 1)\n",
        "    x_t = torch.sqrt(alpha_bars_t) * x_0 + torch.sqrt(1 - alpha_bars_t) * epsilon\n",
        "    t_normalized = (t - 1) / (num_ts - 1)\n",
        "    epsilon_pred = unet(x_t, c, t_normalized, mask)\n",
        "\n",
        "    loss = F.mse_loss(epsilon_pred, epsilon)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMW5YeCi8cqO"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def ddpm_sample(\n",
        "    unet: ClassConditionalUNet,\n",
        "    ddpm_schedule: dict,\n",
        "    c: torch.Tensor,\n",
        "    img_wh: tuple[int, int],\n",
        "    num_ts: int,\n",
        "    guidance_scale: float = 5.0,\n",
        "    seed: int = 0,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Algorithm 4 of the DDPM paper - Class-conditioned sampling with classifier-free guidance.\n",
        "\n",
        "    Args:\n",
        "        unet: ClassConditionalUNet\n",
        "        ddpm_schedule: dict\n",
        "        c: (N,) int64 condition tensor - class labels\n",
        "        img_wh: (H, W) output image width and height.\n",
        "        num_ts: int, number of timesteps.\n",
        "        guidance_scale: float, classifier-free guidance scale (γ).\n",
        "        seed: int, random seed.\n",
        "\n",
        "    Returns:\n",
        "        (N, C, H, W) final sample.\n",
        "        (N, T_animation, C, H, W) caches.\n",
        "    \"\"\"\n",
        "    unet.eval()\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = next(unet.parameters()).device\n",
        "    batch_size = c.shape[0]\n",
        "\n",
        "    x_t = torch.randn(batch_size, unet.in_channels, *img_wh, device=device)\n",
        "    betas = ddpm_schedule['betas']\n",
        "    alphas = ddpm_schedule['alphas']\n",
        "    alpha_bars = ddpm_schedule['alpha_bars']\n",
        "\n",
        "    all_samples = [x_t.cpu()]\n",
        "\n",
        "    for t in range(num_ts, 0, -1):\n",
        "        t_normalized = torch.full((batch_size,), (t - 1) / (num_ts - 1), device=device)\n",
        "        z = torch.randn_like(x_t) if t > 1 else torch.zeros_like(x_t)\n",
        "        zero_mask = torch.zeros(batch_size, device=device)\n",
        "        epsilon_u = unet(x_t, c, t_normalized, zero_mask)\n",
        "\n",
        "        ones_mask = torch.ones(batch_size, device=device)\n",
        "        epsilon_c = unet(x_t, c, t_normalized, ones_mask)\n",
        "\n",
        "        epsilon = epsilon_u + guidance_scale * (epsilon_c - epsilon_u)\n",
        "\n",
        "        alpha_t = alphas[t - 1]\n",
        "        alpha_bar_t = alpha_bars[t - 1]\n",
        "        beta_t = betas[t - 1]\n",
        "\n",
        "        x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon) / torch.sqrt(alpha_bar_t)\n",
        "        x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
        "\n",
        "        if t > 1:\n",
        "            alpha_bar_t_minus_1 = alpha_bars[t - 2]\n",
        "            coef1 = torch.sqrt(alpha_bar_t_minus_1) * beta_t / (1 - alpha_bar_t)\n",
        "            coef2 = torch.sqrt(alpha_t) * (1 - alpha_bar_t_minus_1) / (1 - alpha_bar_t)\n",
        "            x_t = coef1 * x_0_pred + coef2 * x_t + torch.sqrt(beta_t) * z\n",
        "        else:\n",
        "            x_t = x_0_pred\n",
        "\n",
        "        if t % (num_ts // 10) == 0 or t == 1:\n",
        "            all_samples.append(x_t.cpu())\n",
        "\n",
        "    final_samples = x_t\n",
        "    all_samples = torch.stack(all_samples, dim=1)\n",
        "\n",
        "    return final_samples, all_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdQFWwIt8mXh"
      },
      "outputs": [],
      "source": [
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unet: ClassConditionalUNet,\n",
        "        betas: tuple[float, float] = (1e-4, 0.02),\n",
        "        num_ts: int = 300,\n",
        "        p_uncond: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.unet = unet\n",
        "        self.betas = betas\n",
        "        self.num_ts = num_ts\n",
        "        self.p_uncond = p_uncond\n",
        "        self.ddpm_schedule = nn.ParameterDict(ddpm_schedule(betas[0], betas[1], num_ts))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (N, C, H, W) input tensor.\n",
        "            c: (N,) int64 condition tensor.\n",
        "\n",
        "        Returns:\n",
        "            (,) diffusion loss.\n",
        "        \"\"\"\n",
        "        return ddpm_forward(\n",
        "            self.unet, self.ddpm_schedule, x, c, self.p_uncond, self.num_ts\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def sample(\n",
        "        self,\n",
        "        c: torch.Tensor,\n",
        "        img_wh: tuple[int, int],\n",
        "        guidance_scale: float = 5.0,\n",
        "        seed: int = 0,\n",
        "    ):\n",
        "        return ddpm_sample(\n",
        "            self.unet, self.ddpm_schedule, c, img_wh, self.num_ts, guidance_scale, seed\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_class_conditioned_samples(model, epoch, num_rows=4, guidance_scale=5.0, seed=None):\n",
        "    #for visualization\n",
        "    device = next(model.parameters()).device\n",
        "    if seed is None:\n",
        "        seed = epoch\n",
        "    plt.figure(figsize=(12, num_rows * 1.2))\n",
        "    all_samples = []\n",
        "    for digit in range(10):\n",
        "        digit_samples = []\n",
        "        for row in range(num_rows):\n",
        "            c = torch.tensor([digit], device=device)\n",
        "            samples, _ = model.sample(\n",
        "                c,\n",
        "                img_wh=(28, 28),\n",
        "                guidance_scale=guidance_scale,\n",
        "                seed=seed + row * 100\n",
        "            )\n",
        "\n",
        "            samples = (samples + 1) / 2\n",
        "\n",
        "            digit_samples.append(samples[0, 0].cpu().numpy())\n",
        "\n",
        "    fig, axs = plt.subplots(num_rows, 10, figsize=(10, num_rows), facecolor='black')\n",
        "\n",
        "    fig.patch.set_facecolor('black')\n",
        "\n",
        "    for digit in range(10):\n",
        "        for row in range(num_rows):\n",
        "            c = torch.tensor([digit], device=device)\n",
        "            samples, _ = model.sample(c, img_wh=(28, 28), guidance_scale=guidance_scale, seed=seed + row * 100 + digit)\n",
        "            sample = (samples[0, 0].cpu().numpy() + 1) / 2\n",
        "\n",
        "            axs[row, digit].imshow(sample, cmap='gray')\n",
        "            axs[row, digit].axis('off')\n",
        "\n",
        "            if row == 0:\n",
        "                axs[row, digit].set_title(str(digit), color='white')\n",
        "\n",
        "    plt.suptitle(f\"Epoch {epoch}\", color='white', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig(f\"class_conditioned_samples_epoch_{epoch}.png\", facecolor='black')\n",
        "    plt.show()\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "JiVtDC7HpQ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-Sn617TlSJ"
      },
      "source": [
        "## Training the Class-conditioned UNet\n",
        "\n",
        "- Plot the loss curve\n",
        "- Sample results on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh4Wt3ejTlSJ"
      },
      "outputs": [],
      "source": [
        "def train_class_conditioned_unet(num_epochs=20, batch_size=128, guidance_scale=5.0):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0.5, 0.5)\n",
        "    ])\n",
        "    train_dataset = MNIST(root='data', download=True, transform=transform, train=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = ClassConditionalUNet(in_channels=1, num_classes=10, num_hiddens=64).to(device)\n",
        "\n",
        "    ddpm = DDPM(model, betas=(1e-4, 0.02), num_ts=300, p_uncond=0.1).to(device)\n",
        "    optimizer = optim.Adam(ddpm.parameters(), lr=1e-3)\n",
        "\n",
        "    gamma = 0.1**(1.0 / num_epochs)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "    batch_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ddpm.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (clean_images, labels) in enumerate(train_loader):\n",
        "            clean_images = clean_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            loss = ddpm(clean_images, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        print(f'Epoch: {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.6f}')\n",
        "\n",
        "        if epoch in [0, 4, 9, 14, 19]:\n",
        "            with torch.no_grad():\n",
        "                ddpm.eval()\n",
        "\n",
        "                visualize_class_conditioned_samples(ddpm, epoch+1, num_rows=4, guidance_scale=guidance_scale)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(batch_losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return ddpm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_class_conditioned_unet(num_epochs=20, batch_size=128, guidance_scale=5.0)"
      ],
      "metadata": {
        "id": "DG-a2cAAxG3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio tqdm"
      ],
      "metadata": {
        "id": "_nTWd_YX2UQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import imageio\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_generation_gif(model, num_epochs=20, output_dir=\"generation_gifs\", digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
        "    \"\"\"\n",
        "    Create GIFs showing the generation of digits from noise over multiple epochs.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    frames_by_digit = {digit: [] for digit in digits}\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Processing epoch {epoch+1}/{num_epochs}\")\n",
        "        for digit in digits:\n",
        "            c = torch.tensor([digit], device=device)\n",
        "            samples, animation = model.sample(c, img_wh=(28, 28), guidance_scale=5.0, seed=epoch)\n",
        "            animation = (animation + 1) / 2\n",
        "            final_frame = animation[0, -1, 0].cpu().numpy()\n",
        "            fig, ax = plt.subplots(figsize=(3, 3))\n",
        "            ax.imshow(final_frame, cmap='gray')\n",
        "            ax.set_title(f\"Digit {digit}, Epoch {epoch+1}\")\n",
        "            ax.axis('off')\n",
        "            fig.canvas.draw()\n",
        "            frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
        "            plt.close(fig)\n",
        "            frames_by_digit[digit].append(frame)\n",
        "\n",
        "    for digit, frames in frames_by_digit.items():\n",
        "        print(f\"Creating GIF for digit {digit}\")\n",
        "        gif_path = os.path.join(output_dir, f\"digit_{digit}_evolution.gif\")\n",
        "\n",
        "        imageio.mimsave(gif_path, frames, duration=0.5)#create with imageio\n",
        "        print(f\"Saved GIF to {gif_path}\")\n",
        "\n",
        "    print(\"Creating combined GIF with all digits\")\n",
        "    combined_frames = []\n",
        "    for epoch in range(num_epochs):\n",
        "        fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, digit in enumerate(digits):\n",
        "            digit_frame = frames_by_digit[digit][epoch]\n",
        "            axes[i].imshow(digit_frame)\n",
        "            axes[i].set_title(f\"Digit {digit}\")\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        fig.suptitle(f\"Epoch {epoch+1}\")\n",
        "        fig.tight_layout()\n",
        "        fig.canvas.draw()\n",
        "        combined_frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
        "        plt.close(fig)\n",
        "\n",
        "        combined_frames.append(combined_frame)\n",
        "\n",
        "    combined_gif_path = os.path.join(output_dir, \"all_digits_evolution.gif\")\n",
        "    imageio.mimsave(combined_gif_path, combined_frames, duration=1.0)\n",
        "    print(f\"Saved combined GIF to {combined_gif_path}\")\n",
        "\n",
        "    return frames_by_digit"
      ],
      "metadata": {
        "id": "m-q7UDog2XWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_diffusion_process_gif(model, digit=5, output_dir=\"diffusion_gifs\", epochs=[0, 4, 9, 14, 19]):\n",
        "    \"\"\"\n",
        "    Create a GIF showing the diffusion process (noise to image) for a specific digit.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for epoch in epochs:\n",
        "        print(f\"Creating diffusion process GIF for epoch {epoch+1}\")\n",
        "        c = torch.tensor([digit], device=device)\n",
        "        samples, animation = model.sample(c, img_wh=(28, 28), guidance_scale=5.0, seed=epoch)\n",
        "        animation = (animation + 1) / 2\n",
        "\n",
        "        frames = []\n",
        "        num_frames = animation.shape[1]\n",
        "\n",
        "        for t in range(num_frames):\n",
        "            fig, ax = plt.subplots(figsize=(3, 3))\n",
        "            ax.imshow(animation[0, t, 0].cpu().numpy(), cmap='gray')\n",
        "\n",
        "            if t == 0:\n",
        "                stage = \"Noise\"\n",
        "            elif t == num_frames - 1:\n",
        "                stage = \"Final\"\n",
        "            else:\n",
        "                stage = f\"Step {t}\"\n",
        "\n",
        "            ax.set_title(f\"Digit {digit}: {stage}\")\n",
        "            ax.axis('off')\n",
        "            fig.canvas.draw()\n",
        "            frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
        "            plt.close(fig)\n",
        "\n",
        "            frames.append(frame)\n",
        "\n",
        "        gif_path = os.path.join(output_dir, f\"digit_{digit}_process_epoch_{epoch+1}.gif\")\n",
        "        imageio.mimsave(gif_path, frames, duration=0.1)# this uses 0.1 sec per frame\n",
        "        print(f\"Saved GIF to {gif_path}\")"
      ],
      "metadata": {
        "id": "Pq43VIEP2gx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_generation_gif(model, num_epochs=20)"
      ],
      "metadata": {
        "id": "pJLslh2lPY50"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}